{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freespace Segmentation using TAO UNET\n",
    "\n",
    "MIT License\n",
    " SPDX-FileCopyrightText: Copyright (c) 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    " SPDX-License-Identifier: MIT\n",
    "\n",
    " Permission is hereby granted, free of charge, to any person obtaining a\n",
    " copy of this software and associated documentation files (the \"Software\"),\n",
    " to deal in the Software without restriction, including without limitation\n",
    " the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
    " and/or sell copies of the Software, and to permit persons to whom the\n",
    " Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    " The above copyright notice and this permission notice shall be included in\n",
    " all copies or substantial portions of the Software.\n",
    "\n",
    " THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
    " THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
    " FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
    " DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
    "\n",
    "* Collect synthetic dataset using Isaac Sim for freespace segmentation\n",
    "* Use a pretrained \"peoplesemseg\" model from NVIDIA NGC and train on Isaac Sim generated dataset\n",
    "* Use NVIDIA TAO toolkit to reduce inference latency using method such as pruning and post training quantization\n",
    "* Run Inference on the trained and optimized model and visualize the inferences\n",
    "* Export the trained model to a .etlt file for deployment with Isaac ROS\n",
    "* Run inference on the exported .etlt model to verify deployment using TensorRT\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n",
    "\n",
    "Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "![Freespace Seg workflow](images/intor_to_workflow.png)\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "This notebook shows an example use case of UNet Binary Semantic Segmentation using Train Adapt Optimize (TAO) Toolkit.\n",
    "\n",
    "0. [Set up env variables ](#head-0)\n",
    "1. [Installing the TAO launcher ](#head-1)\n",
    "2. [Prepare dataset and pre-trained model ](#head-2)\n",
    "\t1. [Set up Isaac Sim ](#head-2-1)\n",
    "\t2. [Synthetic Data Generation ](#head-2-2)\n",
    "\t3. [ Visualize the Groundtruth Masks ](#head-2-3)\n",
    "\t4. [Post Processing on Synthetic Data ](#head-2-4)\n",
    "\t5. [Download pre-trained model ](#head-2-5)\n",
    "3. [Provide freespace segmentation training specification ](#head-3)\n",
    "4. [Run TAO training ](#head-4)\n",
    "5. [Evaluate trained models ](#head-5)\n",
    "6. [Visualizing Inferences ](#head-6)\n",
    "7. [Prune the trained model ](#head-7)\n",
    "8. [Retrain the pruned model ](#head-8)\n",
    "9. [Evaluate the retrained model ](#head-9)\n",
    "10. [Fine-Tune on Real - World Data ](#head-10)\n",
    "11. [Evaluate the retrained model ](#head-11)\n",
    "12. [Model Export ](#head-12)\n",
    "13. [Verify the deployed model ](#head-13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set up env variables <a class=\"anchor\" id=\"head-0\"></a>\n",
    "\n",
    "When using the purpose-built pretrained models from NGC, please make sure to set the `$KEY` environment variable to the key as mentioned in the model overview. Failing to do so, can lead to errors when trying to load them as pretrained models.\n",
    "\n",
    "In this experiment; synthitically generated data using NVIDIA Isaac Sim will be stored at `$DATA_DOWNLOAD_DIR` and we will split it in training and validation set. `$USER_EXPERIMENT_DIR` will be used for inputs and outputs models or logs from train, prune, eval experiments. \n",
    "\n",
    "*Note: Please make sure to remove any stray artifacts/files from the `$USER_EXPERIMENT_DIR` or `$DATA_DOWNLOAD_DIR` paths, that may have been generated from previous experiments. Having checkpoint files etc may interfere with creating a training graph for a new experiment.*\n",
    "\n",
    "*Note: This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up env variables for cleaner command line commands.\n",
    "import os\n",
    "\n",
    "%set_env KEY=tlt_encode\n",
    "%set_env GPU_INDEX=0\n",
    "%set_env USER_EXPERIMENT_DIR=/workspace/tao-experiments/freespace\n",
    "%set_env DATA_DOWNLOAD_DIR=/workspace/tao-experiments/data\n",
    "\n",
    "# Set this path if you don't run the notebook from the samples directory.\n",
    "%env NOTEBOOK_ROOT=/home/karma/synthetic_data/freespace_main/freespace_seg\n",
    "\n",
    "# Please define this local project directory that needs to be mapped to the TAO docker session.\n",
    "# The dataset expected to be present in $LOCAL_PROJECT_DIR/data, while the results for the steps\n",
    "# in this notebook will be stored at $LOCAL_PROJECT_DIR/unet\n",
    "# !PLEASE MAKE SURE TO UPDATE THIS PATH!.\n",
    "%env LOCAL_PROJECT_DIR=/home/karma/tao_experiments\n",
    "\n",
    "# !PLEASE MAKE SURE TO UPDATE THIS PATH!.\n",
    "# Point to the 'deps' folder in samples from where you are launching notebook inside unet folder.\n",
    "%env PROJECT_DIR=/workspace/tao-experiments/robot_freespace_seg_Isaac_TAO/deps\n",
    "\n",
    "os.environ[\"LOCAL_DATA_DIR\"] = os.path.join(\n",
    "    os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()),\n",
    "    \"data\"\n",
    ")\n",
    "os.environ[\"LOCAL_EXPERIMENT_DIR\"] = os.path.join(\n",
    "    os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()),\n",
    "    \"freespace\"\n",
    ")\n",
    "\n",
    "# The sample spec files are present in the same path as the downloaded samples.\n",
    "os.environ[\"LOCAL_SPECS_DIR\"] = os.path.join(\n",
    "    os.getenv(\"NOTEBOOK_ROOT\", os.getcwd()),\n",
    "    \"specs\"\n",
    ")\n",
    "%set_env SPECS_DIR=/workspace/tao-experiments/robot_freespace_seg_Isaac_TAO/specs\n",
    "\n",
    "! ls -l $LOCAL_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below maps the project directory on your local host to a workspace directory in the TAO docker instance, so that the data and the results are mapped from in and out of the docker. For more information please refer to the [launcher instance](https://docs.nvidia.com/tao/tao-toolkit/tao_launcher.html) in the user guide.\n",
    "\n",
    "When running this cell on AWS, update the drive_map entry with the dictionary defined below, so that you don't have permission issues when writing data into folders created by the TAO docker.\n",
    "\n",
    "```json\n",
    "drive_map = {\n",
    "    \"Mounts\": [\n",
    "            # Mapping the data directory\n",
    "            {\n",
    "                \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "                \"destination\": \"/workspace/tao-experiments\"\n",
    "            },\n",
    "            # Mapping the specs directory.\n",
    "            {\n",
    "                \"source\": os.environ[\"LOCAL_SPECS_DIR\"],\n",
    "                \"destination\": os.environ[\"SPECS_DIR\"]\n",
    "            },\n",
    "        ],\n",
    "    \"DockerOptions\": {\n",
    "        \"user\": \"{}:{}\".format(os.getuid(), os.getgid())\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "\n",
    "import json\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "\n",
    "# Define the dictionary with the mapped drives\n",
    "drive_map = {\n",
    "    \"Mounts\": [\n",
    "        # Mapping the data directory\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "            \"destination\": \"/workspace/tao-experiments\"\n",
    "        },\n",
    "        # Mapping the specs directory.\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_SPECS_DIR\"],\n",
    "            \"destination\": os.environ[\"SPECS_DIR\"]\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(drive_map, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing the TAO launcher <a class=\"anchor\" id=\"head-1\"></a>\n",
    "The TAO launcher is a python package distributed as a python wheel listed in the `nvidia-pyindex` python index. You may install the launcher by executing the following cell.\n",
    "\n",
    "Please note that TAO Toolkit recommends users to run the TAO launcher in a virtual env with python 3.6.9. You may follow the instruction in this [page](https://virtualenvwrapper.readthedocs.io/en/latest/install.html) to set up a python virtual env using the `virtualenv` and `virtualenvwrapper` packages. Once you have setup virtualenvwrapper, please set the version of python to be used in the virtual env by using the `VIRTUALENVWRAPPER_PYTHON` variable. You may do so by running\n",
    "\n",
    "```sh\n",
    "export VIRTUALENVWRAPPER_PYTHON=/path/to/bin/python3.x\n",
    "```\n",
    "where x >= 6 and <= 8\n",
    "\n",
    "We recommend performing this step first and then launching the notebook from the virtual environment. In addition to installing TAO python package, please make sure of the following software requirements:\n",
    "* python >=3.6.9 < 3.8.x\n",
    "* docker-ce > 19.03.5\n",
    "* docker-API 1.40\n",
    "* nvidia-container-toolkit > 1.3.0-1\n",
    "* nvidia-container-runtime > 3.4.0-1\n",
    "* nvidia-docker2 > 2.5.0-1\n",
    "* nvidia-driver > 455+\n",
    "\n",
    "Once you have installed the pre-requisites, please log in to the docker registry nvcr.io by following the command below\n",
    "\n",
    "```sh\n",
    "docker login nvcr.io\n",
    "```\n",
    "\n",
    "You will be triggered to enter a username and password. The username is `$oauthtoken` and the password is the API key generated from `ngc.nvidia.com`. Please follow the instructions in the [NGC setup guide](https://docs.nvidia.com/ngc/ngc-overview/index.html#generating-api-key) to generate your own API key.\n",
    "\n",
    "Please note that TAO Toolkit recommends users to run the TAO launcher in a virtual env with python >=3.6.9. You may follow the instruction in this [page](https://virtualenvwrapper.readthedocs.io/en/latest/install.html) to set up a python virtual env using the virtualenv and virtualenvwrapper packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! source ~/Envs/launcher/bin/activate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SKIP this step IF you have already installed the TAO launcher.\n",
    "!pip3 install nvidia-pyindex\n",
    "!pip3 install nvidia-tao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the versions of the TAO launcher\n",
    "!tao info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Set up Isaac Sim <a class=\"anchor\" id=\"head-2-1\"></a>\n",
    "* Follow the installation guide to setup [Isaac Sim](https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/install_basic.html#workstation-setup)\n",
    "* Run the [Omniverse Launcher](https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/install_basic.html#isaac-sim-setup-native-workstation-launcher) and click Launch button in Isaac Sim App Selector\n",
    "* In Isaac Sim App Selector, select the 'Open in File Browser' option which will lead you to folder where installation took place. \n",
    "* Get the path of this folder, it will be needed for running the python scripts for SDG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Synthetic Data Generation <a class=\"anchor\" id=\"head-2-2\"></a>\n",
    "\n",
    "#### We need to set the correct paths for the scripts to generate data in the simple room and warehouse scenario:\n",
    "* Open the script at 'IsaacSim/sdg/simple_room_freespace.sh'\n",
    "* Set the Isaac Sim Path corresponding to the location of the folder found in Step A\n",
    "* The script and YAML path correspond to the location where this project is cloned\n",
    "* The number of images and randomization corresponding to the floor and wall can be set as arguments to the Python Scripts\n",
    "* The correpsonding YAML file contains the objects to add in the scene, texture locations and groups of floor and wall prims\n",
    "\n",
    "Repeat the steps for the 'IsaacSim/sdg/warehouse_freespace.sh' script as well.\n",
    "\n",
    "#### IMPORTANT:\n",
    "Make sure to create floor and wall randomization folders which contain the appropriate images for their textures. For better results, textures should be similar to the real world environment where the model will be deployed. Upload these folders to your Nucleus Server and specify their path in the YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAKE SURE THE PATHS IN THE SCRIPTS ARE SET CORRECTLY AS DESCRIBED ABOVE\n",
    "!chmod +x IsaacSim/sdg/simple_room_freespace.sh\n",
    "!chmod +x IsaacSim/sdg/warehouse_freespace.sh\n",
    "\n",
    "!./IsaacSim/sdg/simple_room_freespace.sh\n",
    "!./IsaacSim/sdg/warehouse_freespace.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.  Visualize the Groundtruth Masks <a class=\"anchor\" id=\"head-2-3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize_images(image_folder):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    columns = 5\n",
    "    img_list = os.listdir(image_folder)\n",
    "    for num in range(0,5):\n",
    "        plt.subplot(2, columns, num + 1)\n",
    "        image_path = img_list[num]\n",
    "        image = mpimg.imread(os.path.join(image_folder,image_path))\n",
    "        plt.imshow(image)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_folder = 'IsaacSim/simple_room_freespace/Viewport/rgb'\n",
    "semantic_folder = 'IsaacSim/simple_room_freespace/Viewport/semantic'\n",
    "visualize_images(rgb_folder)\n",
    "visualize_images(semantic_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Post Processing on Synthetic Data <a class=\"anchor\" id=\"head-2-4\"></a>\n",
    "* Now we need to convert the semantic masks in the format which TAO expects, each pixel is represented by the class ID corresponding to it.\n",
    "* Run the scripts below to post process the data, the viewport directory is where images and labels for each scenario was saved\n",
    "* Change it to the approprate path if you changed the OUTPUT_FOLDER in the previous scripts whicle generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 IsaacSim/utils/postProcess_syntheticData.py --scenario simple_room \\\n",
    "                                                     --viewport_directory IsaacSim/simple_room_freespace/Viewport\n",
    "\n",
    "!python3 IsaacSim/utils/postProcess_syntheticData.py --scenario warehouse \\\n",
    "                                                     --viewport_directory IsaacSim/warehouse_freespace/Viewport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the warehouse data folder\n",
    "warehouse_data = 'IsaacSim/warehouse_freespace'\n",
    "\n",
    "# Path to the simple room data folder\n",
    "simple_room_data = 'IsaacSim/simple_room_freespace'\n",
    "\n",
    "# Path to the folder where training images are stored for TAO\n",
    "train_images = os.path.join(os.environ[\"LOCAL_PROJECT_DIR\"], 'data/train_images')\n",
    "\n",
    "# Path to the folder where training labels are stored for TAO\n",
    "train_labels = os.path.join(os.environ[\"LOCAL_PROJECT_DIR\"], 'data/train_labels')\n",
    "\n",
    "# Create the train images and train labels directories\n",
    "os.mkdir(train_images)\n",
    "os.mkdir(train_labels)\n",
    "\n",
    "# Copy the warehouse and simple room data to the TAO folders\n",
    "!python3 IsaacSim/utils/copy_data.py --sim_source_folder $warehouse_data --tao_train_images_folder $train_images \\\n",
    "                                    --tao_train_labels_folder $train_labels\n",
    "\n",
    "!python3 IsaacSim/utils/copy_data.py --sim_source_folder $simple_room_data --tao_train_images_folder $train_images \\\n",
    "                                    --tao_train_labels_folder $train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to validation images for TAO\n",
    "validation_images = os.path.join(os.environ[\"LOCAL_PROJECT_DIR\"], 'data/validation_images')\n",
    "\n",
    "# Path to validation labels for TAO\n",
    "validation_labels = os.path.join(os.environ[\"LOCAL_PROJECT_DIR\"], 'data/validation_labels')\n",
    "\n",
    "# Create the valdiation directories for TAO\n",
    "os.mkdir(validation_images)\n",
    "os.mkdir(validation_labels)\n",
    "\n",
    "# Split the data for training and validation\n",
    "!python3 IsaacSim/utils/split_data.py --train_images $train_images --train_labels $train_labels \\\n",
    "            --val_images $validation_images --val_labels $validation_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Download pre-trained model <a class=\"anchor\" id=\"head-2-5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use NGC CLI to get the pre-trained models. For more details, go to ngc.nvidia.com and click the SETUP on the navigation bar. \n",
    "\n",
    "We will be using `peoplesemseg` model based on vanilla unet segementation. We download `trainable` model from NVIDIA NGC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/pretrained_peoplesemseg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull pretrained model from NGC\n",
    "\n",
    "!wget --quiet --show-progress --progress=bar:force:noscroll --auth-no-challenge --no-check-certificate \\\n",
    "        https://api.ngc.nvidia.com/v2/models/nvidia/tao/peoplesemsegnet/versions/trainable_v1.0/files/peoplesemsegnet.tlt \\\n",
    "        -P  $LOCAL_EXPERIMENT_DIR/pretrained_peoplesemseg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check that model is downloaded into dir.\")\n",
    "!ls -l $LOCAL_EXPERIMENT_DIR/pretrained_peoplesemseg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Provide freespace segmentation training specification <a class=\"anchor\" id=\"head-3\"></a>\n",
    "\n",
    "* Images and Masks path\n",
    "    * In order to use the newly generated images, masks folder update the dataset_config parameter in the spec file at `$SPECS_DIR/spec_vanilla_unet_train.txt` \n",
    "    * Update the train, val images and masks paths. The test only requires the images path. \n",
    "* Pre-trained models\n",
    "* Augmentation parameters for on the fly data augmentation\n",
    "* Other training (hyper-)parameters such as batch size, number of epochs, learning rate etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat $LOCAL_SPECS_DIR/spec_vanilla_unet_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run TAO training <a class=\"anchor\" id=\"head-4\"></a>\n",
    "* Provide the sample spec file and the output directory location for models\n",
    "* WARNING: training will take several hours or one day to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For multi-GPU, change --gpus based on your machine.\")\n",
    "!tao unet train --gpus=1 --gpu_index=$GPU_INDEX \\\n",
    "              -e $SPECS_DIR/spec_vanilla_unet_train.txt \\\n",
    "              -r $USER_EXPERIMENT_DIR/semseg_experiment_unpruned \\\n",
    "              -m  $USER_EXPERIMENT_DIR/pretrained_peoplesemseg/peoplesemsegnet.tlt \\\n",
    "              -n freespace_unpruned \\\n",
    "              -k tlt_encode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unet supports restarting from checkpoint. In case, the training job is killed prematurely, you may resume training from the closest checkpoint by simply re-running the same command line. Please do make sure to use the same number of GPUs when restarting the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model for every epoch at checkpoint_interval mentioned in the spec file:')\n",
    "print('---------------------')\n",
    "!ls -ltrh $LOCAL_EXPERIMENT_DIR/semseg_experiment_unpruned/\n",
    "!ls -ltrh $LOCAL_EXPERIMENT_DIR/semseg_experiment_unpruned/weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate trained models <a class=\"anchor\" id=\"head-5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step model saved in the `$USER_EXPERIMENT_DIR/isbi_experiment_unpruned/weights` dir is used for evaluation/ inference/ export. The evaluation also creates `$LOCAL_EXPERIMENT_DIR/isbi_experiment_unpruned/results_tlt.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao unet evaluate --gpu_index=$GPU_INDEX -e $SPECS_DIR/spec_vanilla_unet_train.txt \\\n",
    "                 -m $USER_EXPERIMENT_DIR/semseg_experiment_unpruned/weights/freespace_unpruned.tlt\\\n",
    "                 -o $USER_EXPERIMENT_DIR/semseg_experiment_unpruned/ \\\n",
    "                 -k tlt_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $LOCAL_EXPERIMENT_DIR/semseg_experiment_unpruned/results_tlt.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing Inferences <a class=\"anchor\" id=\"head-6\"></a>\n",
    "In this section, we run the UNet inference tool to generate inferences on the trained models and print the results. \n",
    "\n",
    "The following cell will run inference for segmentation and visualize masks for the images in test. The resulting visualized images will be saved in the `vis_overlay_tlt` folder and label PNG masks in `mask_labels_tlt` in the path provided to `-o` argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao unet inference --gpu_index=$GPU_INDEX -e $SPECS_DIR/spec_vanilla_unet_train.txt \\\n",
    "                 -m $USER_EXPERIMENT_DIR/semseg_experiment_unpruned/weights/freespace_unpruned.tlt \\\n",
    "                 -o $USER_EXPERIMENT_DIR/semseg_experiment_unpruned/ \\\n",
    "                 -k $KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sample images.\n",
    "inference_folder = os.environ[\"LOCAL_EXPERIMENT_DIR\"] + '/semseg_experiment_unpruned/vis_overlay_tlt' \n",
    "\n",
    "visualize_images(inference_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prune the trained model <a class=\"anchor\" id=\"head-7\"></a>\n",
    "* Specify pre-trained model\n",
    "* Equalization criterion\n",
    "* Threshold for pruning.\n",
    "* A key to save and load the model\n",
    "* Output directory to store the model\n",
    "\n",
    "*Usually, you just need to adjust `-pth` (threshold) for accuracy and model size trade off. Higher `pth` gives you smaller model (and thus higher inference speed) but worse accuracy. The threshold to use is dependent on the dataset. A pth value `5.2e-6` is just a start point. If the retrain accuracy is good, you can increase this value to get smaller models. Otherwise, lower this value to get better accuracy.*\n",
    "\n",
    "* For some internal studies, we have noticed that a pth value of 0.1 is a good starting point for unet models trained on larger datasets. A larger regularization value in the first round of training will result in smaller models while pruning. Hence regularization while training and pth are hyper-parameters that needs to be tuned.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output directory if it doesn't exist.\n",
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/semseg_experiment_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao unet prune \\\n",
    "                  -e $SPECS_DIR/spec_vanilla_unet_train.txt \\\n",
    "                  -m $USER_EXPERIMENT_DIR/semseg_experiment_unpruned/weights/freespace_unpruned.tlt \\\n",
    "                  -o $USER_EXPERIMENT_DIR/semseg_experiment_pruned/model_freespace_pruned.tlt \\\n",
    "                  -eq union \\\n",
    "                  -pth 0.1 \\\n",
    "                  -k $KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -rlt $LOCAL_EXPERIMENT_DIR/semseg_experiment_pruned/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Retrain the pruned model <a class=\"anchor\" id=\"head-8\"></a>\n",
    "* Model needs to be re-trained to bring back accuracy after pruning\n",
    "* Specify re-training specification with pretrained weights as pruned model.\n",
    "\n",
    "*Note: For retraining, please set the `load_graph` option to `true` in the model_config to load the pruned model graph. Also, if after retraining, the model shows some decrease in MIOU, it could be that the originally trained model was pruned a little too much. Please try reducing the pruning threshold (thereby reducing the pruning ratio) and use the new model to retrain.*\n",
    "\n",
    "*Note: Ensure to provide a different folder for saving results of retraining from the folder where pruned model is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the retrain experiment file. \n",
    "# Note: We have updated the experiment file to include the \n",
    "# newly pruned model as a pretrained weights and, the\n",
    "# load_graph option is set to true \n",
    "!cat $LOCAL_SPECS_DIR/spec_vanilla_unet_prune_retrain.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retraining using the pruned model as pretrained weights \n",
    "!tao unet train --gpus=1 --gpu_index=$GPU_INDEX \\\n",
    "              -e $SPECS_DIR/spec_vanilla_unet_prune_retrain.txt \\\n",
    "              -r $USER_EXPERIMENT_DIR/semseg_experiment_retrain \\\n",
    "              -m $USER_EXPERIMENT_DIR/semseg_experiment_pruned/model_freespace_pruned.tlt \\\n",
    "              -n model_freespace_retrained \\\n",
    "              -k $KEY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the newly retrained model.\n",
    "!ls -rlt $LOCAL_EXPERIMENT_DIR/semseg_experiment_retrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate the retrained model <a class=\"anchor\" id=\"head-9\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao unet evaluate --gpu_index=$GPU_INDEX -e $SPECS_DIR/spec_vanilla_unet_prune_retrain.txt \\\n",
    "                 -m $USER_EXPERIMENT_DIR/semseg_experiment_retrain/weights/model_freespace_retrained.tlt \\\n",
    "                 -o $USER_EXPERIMENT_DIR/semseg_experiment_retrain/ \\\n",
    "                 -k $KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao unet inference --gpu_index=$GPU_INDEX -e $SPECS_DIR/spec_vanilla_unet_prune_retrain.txt \\\n",
    "                 -m $USER_EXPERIMENT_DIR/semseg_experiment_retrain/weights/model_freespace_retrained.tlt \\\n",
    "                 -o $USER_EXPERIMENT_DIR/semseg_experiment_retrain/validation_images \\\n",
    "                 -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Fine-Tune on Real - World Data <a class=\"anchor\" id=\"head-10\"></a>\n",
    "* For completing this step, you will need few labeled real world images\n",
    "* The format should be same as the data provided earlier with TAO (Images and Labels in Image format)\n",
    "* Place the folders in the Local Experiments Train Directory, dataset paths should be updated correspondingly in the spec file which will be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao unet train --gpus=1 --gpu_index=$GPU_INDEX \\\n",
    "              -e $SPECS_DIR/spec_vanilla_unet_rw_finetune.txt \\\n",
    "              -r $USER_EXPERIMENT_DIR/semseg_experiment_rw_finetune \\\n",
    "              -m $USER_EXPERIMENT_DIR/semseg_experiment_retrain/weights/model_freespace_retrained.tlt \\\n",
    "              -n model_freespace_rw_finetune \\\n",
    "              -k $KEY "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluate the retrained model <a class=\"anchor\" id=\"head-11\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section evaluates the pruned and retrained model, using the `evaluate` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao unet evaluate --gpu_index=$GPU_INDEX -e $SPECS_DIR/spec_vanilla_unet_rw_finetune.txt \\\n",
    "                 -m $USER_EXPERIMENT_DIR/semseg_experiment_rw_finetune/weights/model_freespace_rw_finetune.tlt \\\n",
    "                 -o $USER_EXPERIMENT_DIR/semseg_experiment_rw_finetune/ \\\n",
    "                 -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Export <a class=\"anchor\" id=\"head-12\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tao <task> export will fail if .etlt already exists. So we clear the export folder before tao <task> export\n",
    "!rm -rf $LOCAL_EXPERIMENT_DIR/export\n",
    "# # Export in FP32 mode. \n",
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/export \n",
    "\n",
    "!tao unet export --gpu_index=$GPU_INDEX -m $USER_EXPERIMENT_DIR/semseg_experiment_rw_finetune/weights/model_freespace_rw_finetune.tlt \\\n",
    "                -o $USER_EXPERIMENT_DIR/export/model_freespace_rw_finetune.etlt \\\n",
    "                -k $KEY \\\n",
    "                -e $SPECS_DIR/spec_vanilla_unet_rw_finetune.txt \\\n",
    "                --data_type int8 \\\n",
    "                --batches 10 \\\n",
    "                --cal_image_dir $DATA_DOWNLOAD_DIR/real_world_images \\\n",
    "                --engine_file $USER_EXPERIMENT_DIR/export/model_freespace_rw_finetune_int8.engine \\\n",
    "                --cal_cache_file  $USER_EXPERIMENT_DIR/export/model_freespace_rw_finetune_int8_cal.bin \\\n",
    "                --cal_data_file $USER_EXPERIMENT_DIR/export/model_freespace_rw_finetune_int8_data.txt \\\n",
    "                --max_batch_size 4 \\\n",
    "                --batch_size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if etlt model is correctly saved.\n",
    "!ls -l $LOCAL_EXPERIMENT_DIR/export/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify engine generation using the `tao-converter` utility included in the docker.\n",
    "\n",
    "The `tao-converter` produces optimized tensorrt engines for the platform that it resides on. Therefore, to get maximum performance, please instantiate this docker and execute the `tao-converter` command, with the exported `.etlt` file and calibration cache (for int8 mode) on your target device. The tao-converter utility included in this docker only works for x86 devices, with discrete NVIDIA GPU's. \n",
    "\n",
    "For the jetson devices, please download the tao-converter for jetson from the dev zone link [here](https://developer.nvidia.com/tao-converter). \n",
    "\n",
    "If you choose to integrate your model into deepstream directly, please refer to [deepstream dev guide](https://docs.nvidia.com/metropolis/deepstream/dev-guide/index.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao converter -k $KEY  \\\n",
    "               -e $USER_EXPERIMENT_DIR/export/model_freespace_rw_finetune_int8_trt.engine \\\n",
    "               -c $USER_EXPERIMENT_DIR/export/model_freespace_rw_finetune_int8_cal.bin \\\n",
    "               -t int8 \\\n",
    "               -p input_1,1x3x512x512,2x3x512x512,4x3x512x512 \\\n",
    "               -m $USER_EXPERIMENT_DIR/export/model_freespace_rw_finetune.etlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Exported engine:')\n",
    "print('------------')\n",
    "!ls -lh $LOCAL_EXPERIMENT_DIR/export/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Verify the deployed model <a class=\"anchor\" id=\"head-13\"></a>\n",
    "\n",
    "Verify the converted engine. The resulting TRT inference images will be saved in `vis_overlay_trt` folder and PNG masks in `mask_labels_trt`  in the path provided to `-o` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao unet inference --gpu_index=$GPU_INDEX -e $SPECS_DIR/spec_vanilla_unet_rw_finetune.txt \\\n",
    "                 -m $USER_EXPERIMENT_DIR/export/model_freespace_rw_finetune_int8.engine \\\n",
    "                 -o $USER_EXPERIMENT_DIR/semseg_experiment_rw_finetune/ \\\n",
    "                 -k $KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao unet evaluate --gpu_index=$GPU_INDEX -e $SPECS_DIR/spec_vanilla_unet_rw_finetune.txt \\\n",
    "                 -m $USER_EXPERIMENT_DIR/export/model_freespace_rw_finetune_int8.engine \\\n",
    "                 -o $USER_EXPERIMENT_DIR/semseg_experiment_rw_finetune/ \\\n",
    "                 -k $KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sample images.\n",
    "inference_folder = os.environ[\"LOCAL_EXPERIMENT_DIR\"] + '/semseg_experiment_re_finetune/vis_overlay_tlt' \n",
    "\n",
    "visualize_images(inference_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets deploy trained model with real world images on Jetson with Isaac ROS. Steps are included in README on GitHub.\n",
    "\n",
    "![Isaac ROS Output](images/isaac_ros_output.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
